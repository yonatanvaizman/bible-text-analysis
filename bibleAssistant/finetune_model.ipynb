{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2e834e6",
   "metadata": {},
   "source": [
    "# Fine tune an LLM for Bible Assistant AI agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39284646",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c384d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "current_dir = Path().resolve()\n",
    "sys.path.append(str(current_dir.parent))\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "import huggingface_hub\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import ollama\n",
    "\n",
    "import bibleAssistant.agent as bagent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "311c13f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "# For debugging pytorch environment\n",
    "x = torch.tensor([1,2,3])\n",
    "print(x.numpy())   # should work now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48df5c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.4.1+cpu\n",
      "Transformers: 4.57.3\n",
      "Accelerate: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import transformers, accelerate, torch\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Accelerate:\", accelerate.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f41f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub.login(os.environ[\"HUGGING_FACE_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323d9758",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abfeeb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the possible data files (in c:\\Users\\Yonatan\\Documents\\coding_projects\\bible-text-analysis\\data\\dev):\n",
      "['lookup_verse.3.test.jsonl', 'lookup_verse.3.train.jsonl', 'two_tools.1.test.jsonl', 'two_tools.1.train.jsonl']\n",
      "----------------------------------------\n",
      "What output model folders we already have:\n",
      "['gemma3-lora-8', 'gemma3-lora-9']\n",
      "----------------------------------------\n",
      "Which models are already locally served in ollama:\n",
      "['gemma3:1b-ft9-650', 'gemma3:1b-ft9-350', 'gemma3:1b-ft3-50', 'gemma3:1b-ft3-200', 'gemma3:1b-ft3', 'gemma3:1b-ft2', 'gemma3:1b-ft1', 'google/gemma-3-1b-it-ft1:latest', 'gemma3:4b', 'llama3.2:3b', 'llama3.2:1b', 'gemma3:1b', 'mistral:latest']\n"
     ]
    }
   ],
   "source": [
    "dev_folder = os.path.abspath(\"../data/dev\")\n",
    "print(f\"What are the possible data files (in {dev_folder}):\")\n",
    "print(os.listdir(dev_folder))\n",
    "\n",
    "print(\"-\"*40)\n",
    "print(\"What output model folders we already have:\")\n",
    "print(os.listdir(\"../models\"))\n",
    "\n",
    "print(\"-\"*40)\n",
    "print(\"Which models are already locally served in ollama:\")\n",
    "print([item.model for item in ollama.list().models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccc822d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:\n",
      "Train set from c:\\Users\\Yonatan\\Documents\\coding_projects\\bible-text-analysis\\data\\dev\\two_tools.1.train.jsonl\n",
      "Test  set from c:\\Users\\Yonatan\\Documents\\coding_projects\\bible-text-analysis\\data\\dev\\two_tools.1.test.jsonl\n",
      "Model:\n",
      "  Start from google/gemma-3-1b-it\n",
      "  Save adaptation checkpoints into c:\\Users\\Yonatan\\Documents\\coding_projects\\bible-text-analysis\\models\\gemma3-2tools1\n",
      "  Serve in ollama, based on model gemma3:1b with version of 2t1 (and checkpoint)\n"
     ]
    }
   ],
   "source": [
    "trainset_file = os.path.join(dev_folder, \"two_tools.1.train.jsonl\")\n",
    "testset_file = os.path.join(dev_folder, \"two_tools.1.test.jsonl\")\n",
    "print(f\"Input data:\")\n",
    "print(f\"Train set from {trainset_file}\")\n",
    "print(f\"Test  set from {testset_file}\")\n",
    "\n",
    "# Huggingface, base and adaptation:\n",
    "model_name = \"google/gemma-3-1b-it\" # Huggingface base model\n",
    "output_dir = os.path.abspath(\"../models/gemma3-2tools1\")\n",
    "\n",
    "# For serving the model in ollama:\n",
    "ollama_base_model_name = \"gemma3:1b\"\n",
    "model_nickname = \"2t1\"\n",
    "\n",
    "print(f\"Model:\")\n",
    "print(f\"  Start from {model_name}\")\n",
    "print(f\"  Save adaptation checkpoints into {output_dir}\")\n",
    "print(f\"  Serve in ollama, based on model {ollama_base_model_name} with version of {model_nickname} (and checkpoint)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a5939b",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93304f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_generate(use_tokenizer, use_model, prompt, max_new_tokens=100, temperature=0.7):\n",
    "    inputs = use_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    if temperature > 0:\n",
    "        outputs = use_model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature)\n",
    "    else:\n",
    "        outputs = use_model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    output = outputs[0][input_len:] # Remove the repeated input token sequence. Keep only the newly generated tokens\n",
    "    outtext = use_tokenizer.decode(output, skip_special_tokens=True)\n",
    "    return outtext\n",
    "\n",
    "def do_generate_response(use_tokenizer, use_model, messages, max_new_tokens=100, temperature=0.7):\n",
    "    prompt = use_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return do_generate(use_tokenizer, use_model, prompt, max_new_tokens=max_new_tokens, temperature=temperature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857e6f62",
   "metadata": {},
   "source": [
    "## Load train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bf225f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24df0d2060a044bc8663745ff3f12b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173c35eb0fe345488a0c31e3826bcb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds = load_dataset(\"json\", data_files=trainset_file, split=\"train\")\n",
    "test_ds = load_dataset(\"json\", data_files=testset_file, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a025655",
   "metadata": {},
   "source": [
    "## Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "855481f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37c3ebeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/><div style='display: inline-block; border:3px solid #990099; padding:10px; margin-bottom:5px; border-radius: 5px; margin-left: 5px'><span style='color: #990099'>System:</span><br/>You are a research assistant for biblical texts that always responds using a JSON object with fields \"tool\" and \"arguments\".<br/><br/>To respond normally to the user, use:<br/>{\"tool\": \"respond_to_user\", \"arguments\":{\"text\": \"<text to show the user>\"}}<br/><br/>To call a tool, you need to indicate which tool to use and what arguments to send to it - use the structure:<br/>{\"tool\": \"<tool_name>\", \"arguments\":{ ... }}<br/><br/>After you call a tool, you will receive a tool-response message from role \"user\" containing a JSON object.<br/>The tool-response object always includes fields \"tool_name\" and \"status\".<br/><br/>If the tool call succeeded, the tool response object will have the structure:<br/>{\"tool_name\": \"<tool_name>\", \"status\": \"ok\", \"result\": { ... }}<br/>Different tools have different structures of the returned data inside the dictionary \"result\".<br/>If you know what to do next, go ahead (e.g., if the user already told you what to do with the results, or if you have a plan and you want to use information from the results to do another tool call).<br/>If you're not sure what to do next or how to present the results to the user, you can respond to the user with \"o.k. what now?\" to get further instructions.<br/><br/>If the tool call fails, the tool response object will have the structure:<br/>{\"tool_name\": \"<tool_name>\", \"status\": \"error\", \"error_message\": \" ... \"}<br/>If the error message is clear enough, you can try to fix the problem yourself (e.g., call the same tool with corrected arguments, or call another tool).<br/>Otherwise, you can surface the error message back to the user (with \"respond_to_user\") sto get further instructions.<br/><br/>Available tools:<br/><br/>lookup_verse(version: str, book: str, chapter_num: int, verse_num: int) -> dict<br/>    Get the text of a specific verse from the bible.<br/><br/>    Args:<br/>    - version (str): the code name of a specific bible version or translation<br/>    - book (str): the name of the book from the bible<br/>    - chapter_num (int): chapter number inside the book<br/>    - verse_num (int): verse number inside the chapter<br/><br/>    Returns:<br/>    - dictionary with fields version, book, chapter_num, and verse_num coppied from the input arguments, and an additional field:<br/>        - text (str): the text of the requested verse<br/>    </div><br/><div style='display: inline-block; border:3px solid #00BB00; padding:10px; margin-bottom:5px; border-radius: 5px; margin-left: 5px'><span style='color: #00BB00'>You:</span><br/>Please show deuteronomy 29:18 using the 'en.nwe.jps1917' version.</div><br/><div style='display: inline-block; border:3px solid #00BB88; padding:10px; margin-bottom:5px; border-radius: 5px; margin-left: 100px'><span style='color: #00BB88'>Tool call:</span><br/>Calling <b>lookup_verse</b>(version=en.nwe.jps1917, book=deuteronomy, chapter_num=29, verse_num=18)</div><br/><div style='display: inline-block; border:3px solid #0088BB; padding:10px; margin-bottom:5px; border-radius: 5px; margin-left: 100px'><span style='color: #0088BB'>Tool response:</span><br/>Failed call to <b>lookup_verse</b>: We don't support text-version named 'en.nwe.jps1917'. Here are the supported versions:<br/>  Version: 'he.text_only'. Description: Hebrew language, only letters.<br/>  Version: 'he.masorah'. Description: Hebrew language, traditional script version (full markings).<br/>  Version: 'en.new.jps1917'. Description: English language, JPS.<br/>  Version: 'en.koren'. Description: English language, Koren.</div><br/><div style='display: inline-block; border:3px solid #2222FF; padding:10px; margin-bottom:5px; border-radius: 5px; margin-left: 50px'><span style='color: #2222FF'>Assistant:</span><br/>We don't support text-version named 'en.nwe.jps1917'. Here are the supported versions:<br/>  Version: 'he.text_only'. Description: Hebrew language, only letters.<br/>  Version: 'he.masorah'. Description: Hebrew language, traditional script version (full markings).<br/>  Version: 'en.new.jps1917'. Description: English language, JPS.<br/>  Version: 'en.koren'. Description: English language, Koren.</div><br/><div style='display: inline-block; border:3px solid #00BB00; padding:10px; margin-bottom:5px; border-radius: 5px; margin-left: 5px'><span style='color: #00BB00'>You:</span><br/>let me correct: deuteronomy 29:18 version 'en.new.jps1917'</div><br/><div style='display: inline-block; border:3px solid #00BB88; padding:10px; margin-bottom:5px; border-radius: 5px; margin-left: 100px'><span style='color: #00BB88'>Tool call:</span><br/>Calling <b>lookup_verse</b>(version=en.new.jps1917, book=deuteronomy, chapter_num=29, verse_num=18)</div><br/><div style='display: inline-block; border:3px solid #0088BB; padding:10px; margin-bottom:5px; border-radius: 5px; margin-left: 100px'><span style='color: #0088BB'>Tool response:</span><br/>Response from <b>lookup_verse</b>: {'version': 'en.new.jps1917', 'book': 'deuteronomy', 'chapter_num': 29, 'verse_num': 18, 'text': 'and it come to pass, when he heareth the words of this curse, that he bless himself in his heart, saying: ‘I shall have peace, though I walk in the stubbornness of my heart—that the watered be swept away with the dry’;'}</div><br/><div style='display: inline-block; border:3px solid #2222FF; padding:10px; margin-bottom:5px; border-radius: 5px; margin-left: 50px'><span style='color: #2222FF'>Assistant:</span><br/>and it come to pass, when he heareth the words of this curse, that he bless himself in his heart, saying: ‘I shall have peace, though I walk in the stubbornness of my heart—that the watered be swept away with the dry’;</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bagent.AgentUI().display_convo(train_ds[10]['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "458fdb24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(179)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([len(ex[\"messages\"]) for ex in train_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2dbddec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': 'You are a research assistant for biblical texts that always responds using a JSON object with fields \"tool\" and \"arguments\".\\n\\nTo respond normally to the user, use:\\n{\"tool\": \"respond_to_user\", \"arguments\":{\"text\": \"<text to show the user>\"}}\\n\\nTo call a tool, you need to indicate which tool to use and what arguments to send to it - use the structure:\\n{\"tool\": \"<tool_name>\", \"arguments\":{ ... }}\\n\\nAfter you call a tool, you will receive a tool-response message from role \"user\" containing a JSON object.\\nThe tool-response object always includes fields \"tool_name\" and \"status\".\\n\\nIf the tool call succeeded, the tool response object will have the structure:\\n{\"tool_name\": \"<tool_name>\", \"status\": \"ok\", \"result\": { ... }}\\nDifferent tools have different structures of the returned data inside the dictionary \"result\".\\nIf you know what to do next, go ahead (e.g., if the user already told you what to do with the results, or if you have a plan and you want to use information from the results to do another tool call).\\nIf you\\'re not sure what to do next or how to present the results to the user, you can respond to the user with \"o.k. what now?\" to get further instructions.\\n\\nIf the tool call fails, the tool response object will have the structure:\\n{\"tool_name\": \"<tool_name>\", \"status\": \"error\", \"error_message\": \" ... \"}\\nIf the error message is clear enough, you can try to fix the problem yourself (e.g., call the same tool with corrected arguments, or call another tool).\\nOtherwise, you can surface the error message back to the user (with \"respond_to_user\") sto get further instructions.\\n\\nAvailable tools:\\n\\nsearch_phrase(phrase: str, n_max_results: int = 10) -> dict\\n    Search the bible for all the verses that contain a specific phrase.\\n    Currently supporting Hebrew text only (searching in WLCC version - Westminster Leningrad Codex (Consonants)).\\n\\n    Args:\\n    - phrase (str): the word or phrase to search for. Currently only supporting a phrase in Hebrew without Nikkud (consonants only).\\n    - n_max_results (int): the maximum number of results to return. Default: 10\\n\\n    Returns:\\n    - dictionary with a field \"results\" of the verses that have the phrase - a list items, each is a dictionary with fields:\\n        - book_name (str): the name of the biblical book\\n        - chapter_num (int): the chapter number inside the book\\n        - verse_num (int): the verse number inside the chapter\\n        - text (str): the text of the found verse (this text should include the searched phrase as a substring)\\n    \\n\\n----\\n\\nlookup_verse(version: str, book: str, chapter_num: int, verse_num: int) -> dict\\n    Get the text of a specific verse from the bible.\\n\\n    Args:\\n    - version (str): the code name of a specific bible version or translation\\n    - book (str): the name of the book from the bible\\n    - chapter_num (int): chapter number inside the book\\n    - verse_num (int): verse number inside the chapter\\n\\n    Returns:\\n    - dictionary with fields version, book, chapter_num, and verse_num coppied from the input arguments, and an additional field:\\n        - text (str): the text of the requested verse\\n    '}\n",
      "{'role': 'user', 'content': \"Give me verse 30 from chapter 22 in the 'he.text_only' version of exodus.\"}\n",
      "{'role': 'assistant', 'content': '{\"tool\": \"lookup_verse\", \"arguments\": {\"version\": \"he.text_only\", \"book\": \"exodus\", \"chapter_num\": 22, \"verse_num\": 30}}'}\n",
      "{'role': 'user', 'content': '{\"tool_name\": \"lookup_verse\", \"status\": \"ok\", \"result\": {\"version\": \"he.text_only\", \"book\": \"exodus\", \"chapter_num\": 22, \"verse_num\": 30, \"text\": \"ואנשי קדש תהיון לי ובשר בשדה טרפה לא תאכלו לכלב תשלכון אתו\"}}'}\n",
      "{'role': 'assistant', 'content': '{\"tool\": \"respond_to_user\", \"arguments\": {\"text\": \"ואנשי קדש תהיון לי ובשר בשדה טרפה לא תאכלו לכלב תשלכון אתו\"}}'}\n",
      "{'role': 'user', 'content': \"Give me the text of deuteronomy 10:3 in 'he.masorah'.\"}\n",
      "{'role': 'assistant', 'content': '{\"tool\": \"lookup_verse\", \"arguments\": {\"version\": \"he.masorah\", \"book\": \"deuteronomy\", \"chapter_num\": 10, \"verse_num\": 3}}'}\n",
      "{'role': 'user', 'content': '{\"tool_name\": \"lookup_verse\", \"status\": \"ok\", \"result\": {\"version\": \"he.masorah\", \"book\": \"deuteronomy\", \"chapter_num\": 10, \"verse_num\": 3, \"text\": \"וָאַ֤עַשׂ אֲרוֹן֙ עֲצֵ֣י שִׁטִּ֔ים וָאֶפְסֹ֛ל שְׁנֵי־לֻחֹ֥ת אֲבָנִ֖ים כָּרִאשֹׁנִ֑ים וָאַ֣עַל הָהָ֔רָה וּשְׁנֵ֥י הַלֻּחֹ֖ת בְּיָדִֽי׃\"}}'}\n",
      "{'role': 'assistant', 'content': '{\"tool\": \"respond_to_user\", \"arguments\": {\"text\": \"וָאַ֤עַשׂ אֲרוֹן֙ עֲצֵ֣י שִׁטִּ֔ים וָאֶפְסֹ֛ל שְׁנֵי־לֻחֹ֥ת אֲבָנִ֖ים כָּרִאשֹׁנִ֑ים וָאַ֣עַל הָהָ֔רָה וּשְׁנֵ֥י הַלֻּחֹ֖ת בְּיָדִֽי׃\"}}'}\n",
      "{'role': 'user', 'content': 'and from en.koren.'}\n",
      "{'role': 'assistant', 'content': '{\"tool\": \"lookup_verse\", \"arguments\": {\"version\": \"en.koren\", \"book\": \"DEUTEROOMY\", \"chapter_num\": 10, \"verse_num\": 3}}'}\n",
      "{'role': 'user', 'content': '{\"tool_name\": \"lookup_verse\", \"status\": \"error\", \"error_message\": \"We don\\'t support book named \\'deuteroomy\\'. Here are the supported books: genesis, exodus, leviticus, numbers, deuteronomy, isaiah, jeremiah\"}'}\n",
      "{'role': 'assistant', 'content': '{\"tool\": \"lookup_verse\", \"arguments\": {\"version\": \"en.koren\", \"book\": \"deuteronomy\", \"chapter_num\": 10, \"verse_num\": 3}}'}\n",
      "{'role': 'user', 'content': '{\"tool_name\": \"lookup_verse\", \"status\": \"ok\", \"result\": {\"version\": \"en.koren\", \"book\": \"deuteronomy\", \"chapter_num\": 10, \"verse_num\": 3, \"text\": \"And I made an ark of shittim wood, and hewed two tablets of stone like the first, and went up to the mountain, having the two tablets in my hand.\"}}'}\n",
      "{'role': 'assistant', 'content': '{\"tool\": \"respond_to_user\", \"arguments\": {\"text\": \"And I made an ark of shittim wood, and hewed two tablets of stone like the first, and went up to the mountain, having the two tablets in my hand.\"}}'}\n",
      "{'role': 'user', 'content': \"Please show geness 31:45 using the 'he.text_only' version.\"}\n",
      "{'role': 'assistant', 'content': '{\"tool\": \"lookup_verse\", \"arguments\": {\"version\": \"he.text_only\", \"book\": \"geness\", \"chapter_num\": 31, \"verse_num\": 45}}'}\n",
      "{'role': 'user', 'content': '{\"tool_name\": \"lookup_verse\", \"status\": \"error\", \"error_message\": \"We don\\'t support book named \\'geness\\'. Here are the supported books: genesis, exodus, leviticus, numbers, deuteronomy, isaiah, jeremiah\"}'}\n",
      "{'role': 'assistant', 'content': '{\"tool\": \"lookup_verse\", \"arguments\": {\"version\": \"he.text_only\", \"book\": \"genesis\", \"chapter_num\": 31, \"verse_num\": 45}}'}\n",
      "{'role': 'user', 'content': '{\"tool_name\": \"lookup_verse\", \"status\": \"ok\", \"result\": {\"version\": \"he.text_only\", \"book\": \"genesis\", \"chapter_num\": 31, \"verse_num\": 45, \"text\": \"ויקח יעקב אבן וירימה מצבה\"}}'}\n",
      "{'role': 'assistant', 'content': '{\"tool\": \"respond_to_user\", \"arguments\": {\"text\": \"ויקח יעקב אבן וירימה מצבה\"}}'}\n",
      "{'role': 'user', 'content': 'I want also the en.koren translation.'}\n",
      "{'role': 'assistant', 'content': '{\"tool\": \"lookup_verse\", \"arguments\": {\"version\": \"rn.koren\", \"book\": \"genesis\", \"chapter_num\": 31, \"verse_num\": 45}}'}\n",
      "{'role': 'user', 'content': '{\"tool_name\": \"lookup_verse\", \"status\": \"error\", \"error_message\": \"We don\\'t support text-version named \\'rn.koren\\'. Here are the supported versions:\\\\n  Version: \\'he.text_only\\'. Description: Hebrew language, only letters.\\\\n  Version: \\'he.masorah\\'. Description: Hebrew language, traditional script version (full markings).\\\\n  Version: \\'en.new.jps1917\\'. Description: English language, JPS.\\\\n  Version: \\'en.koren\\'. Description: English language, Koren.\"}'}\n",
      "{'role': 'assistant', 'content': '{\"tool\": \"respond_to_user\", \"arguments\": {\"text\": \"We don\\'t support text-version named \\'rn.koren\\'. Here are the supported versions:\\\\n  Version: \\'he.text_only\\'. Description: Hebrew language, only letters.\\\\n  Version: \\'he.masorah\\'. Description: Hebrew language, traditional script version (full markings).\\\\n  Version: \\'en.new.jps1917\\'. Description: English language, JPS.\\\\n  Version: \\'en.koren\\'. Description: English language, Koren.\"}}'}\n",
      "{'role': 'user', 'content': 'oh sorry. try version en.koren'}\n",
      "{'role': 'assistant', 'content': '{\"tool\": \"lookup_verse\", \"arguments\": {\"version\": \"en.koren\", \"book\": \"genesis\", \"chapter_num\": 31, \"verse_num\": 45}}'}\n",
      "{'role': 'user', 'content': '{\"tool_name\": \"lookup_verse\", \"status\": \"ok\", \"result\": {\"version\": \"en.koren\", \"book\": \"genesis\", \"chapter_num\": 31, \"verse_num\": 45, \"text\": \"And Ya῾aqov took a stone, and set it up for a pillar.\"}}'}\n",
      "----------------------------------------\n",
      "{\"tool\": \"respond_to_user\", \"arguments\": {\"text\": \"And Ya῾aqov took a stone, and set it up for a pillar.\"}}\n"
     ]
    }
   ],
   "source": [
    "input_messages = train_ds[179]['messages'][:-1]\n",
    "for msg in input_messages:\n",
    "    print(msg)\n",
    "print(\"-\"*40)\n",
    "resp = do_generate_response(tokenizer, base_model, input_messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f440b58a",
   "metadata": {},
   "source": [
    "## Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5532b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_for_gemma_token_level(example):\n",
    "    # Convert the list of messages into a single text sequence\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    print(text)\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    labels = [-100] * len(input_ids)\n",
    "\n",
    "    assistant_marker = tokenizer(\"<start_of_turn>model\")[\"input_ids\"][1:] # Assume the first is a <bos> token that will not appear in the middle of a chat\n",
    "    endturn_marker = tokenizer(\"<end_of_turn>\")[\"input_ids\"][1:]\n",
    "    print(f\"assistant marker: {assistant_marker}\")\n",
    "    print(f\"end-turn marker: {endturn_marker}\")\n",
    "    i = 0\n",
    "    while i < len(input_ids):\n",
    "        # Look for assistant start:\n",
    "        if input_ids[i:i+len(assistant_marker)] == assistant_marker:\n",
    "            # Move index to the first token *after* the start marker\n",
    "            i += len(assistant_marker)\n",
    "            # Copy the target tokens until reaching end of assistant message:\n",
    "            while i < len(input_ids) and input_ids[i:i+len(endturn_marker)] != endturn_marker:\n",
    "                labels[i] = input_ids[i]\n",
    "                i += 1\n",
    "            i += len(endturn_marker)\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    tokens[\"labels\"] = labels\n",
    "    return tokens\n",
    "\n",
    "def tokenize_for_gemma(example, max_length=20480): # I increased the max length. Let's see if training gemma can handle it\n",
    "    \"\"\"\n",
    "    Tokenize a multi-turn chat example for Gemma using text-level markers.\n",
    "    Only assistant spans (<start_of_turn>model ... <end_of_turn>) are labeled.\n",
    "    Everything else is masked with -100.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Render the conversation using Gemma's chat template (text only)\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "    # 2. Find all assistant spans in the raw text\n",
    "    assistant_spans = []\n",
    "    start_tag = \"<start_of_turn>model\"\n",
    "    end_tag = \"<end_of_turn>\"\n",
    "\n",
    "    search_pos = 0\n",
    "    while True:\n",
    "        start_idx = text.find(start_tag, search_pos)\n",
    "        if start_idx == -1:\n",
    "            break\n",
    "\n",
    "        # content begins *after* the start tag\n",
    "        content_start = start_idx + len(start_tag)\n",
    "\n",
    "        end_idx = text.find(end_tag, content_start)\n",
    "        if end_idx == -1:\n",
    "            # malformed example; ignore\n",
    "            break\n",
    "\n",
    "        assistant_spans.append((content_start, end_idx))\n",
    "        search_pos = end_idx + len(end_tag)\n",
    "\n",
    "    # 3. Tokenize the entire text with offsets\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    input_ids = encoded[\"input_ids\"]\n",
    "    offsets = encoded[\"offset_mapping\"]\n",
    "\n",
    "    # 4. Initialize all labels as masked\n",
    "    labels = [-100] * len(input_ids)\n",
    "\n",
    "    # 5. For each assistant span, unmask tokens whose offsets fall inside it\n",
    "    for (span_start, span_end) in assistant_spans:\n",
    "        for i, (tok_start, tok_end) in enumerate(offsets):\n",
    "            # token overlaps the assistant span\n",
    "            if tok_end > span_start and tok_start < span_end:\n",
    "                labels[i] = input_ids[i]\n",
    "\n",
    "    encoded[\"labels\"] = labels\n",
    "    encoded.pop(\"offset_mapping\", None)\n",
    "\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc15cb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ec8b93a54546e9a85f726fc3bb4c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tok = train_ds.map(tokenize_for_gemma, remove_columns=train_ds.column_names)\n",
    "test_tok = test_ds.map(tokenize_for_gemma, remove_columns=train_ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92ff1006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging, use small data:\n",
    "# train_tok = train_tok.select(range(20))\n",
    "# test_tok = test_tok.select(range(10))\n",
    "\n",
    "test_tok = test_tok.select(list(range(10)) + list(range(100,130)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "194d8492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Select specific examples by metadata:\n",
    "# train_inds = [i for i in range(len(train_ds)) if train_ds[i]['metadata']['scenario']=='lookup_verse_ok_then_another_version']\n",
    "# print(len(train_inds))\n",
    "# train_tok = train_tok.select(train_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c8056dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 850\n",
      "Test 40\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train {len(train_tok)}\")\n",
    "print(f\"Test {len(test_tok)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "114ad91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tok[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c6b0005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum example length is input: 5544 and label: 5544\n",
      "Maximum example length is input: 3947 and label: 3947\n"
     ]
    }
   ],
   "source": [
    "def validate_tokenized_dataset(tok_ds):\n",
    "    for i, tok_ex in enumerate(tok_ds):\n",
    "        if np.isnan(tok_ex[\"input_ids\"]).any():\n",
    "            print(\"NaN in input_ids at batch\", i)\n",
    "        if np.isnan(tok_ex[\"labels\"]).any():\n",
    "            print(\"NaN in labels at batch\", i)\n",
    "        if (np.array(tok_ex[\"labels\"]) >= tokenizer.vocab_size).any():\n",
    "            print(\"Out-of-range label at batch\", i)\n",
    "        if len(tok_ex[\"labels\"]) <= 0:\n",
    "            print(\"Empty labels at batch\", i)\n",
    "        if i > 100:\n",
    "            break\n",
    "    \n",
    "    max_len = max(len(ex[\"input_ids\"]) for ex in tok_ds)\n",
    "    max_len_label = max(len(ex[\"labels\"]) for ex in tok_ds)\n",
    "    print(f\"Maximum example length is input: {max_len} and label: {max_len_label}\")\n",
    "\n",
    "\n",
    "validate_tokenized_dataset(train_tok)\n",
    "validate_tokenized_dataset(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c2d3d8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_ds)):\n",
    "    if 'another' in train_ds[i]['metadata']['scenario']:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15223698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><bos><start_of_turn>user\n",
      "You are a research assistant for biblical texts that always responds using a JSON object with fields \"tool\" and \"arguments\".\n",
      "\n",
      "To respond normally to the user, use:\n",
      "{\"tool\": \"respond_to_user\", \"arguments\":{\"text\": \"<text to show the user>\"}}\n",
      "\n",
      "To call a tool, you need to indicate which tool to use and what arguments to send to it - use the structure:\n",
      "{\"tool\": \"<tool_name>\", \"arguments\":{ ... }}\n",
      "\n",
      "After you call a tool, you will receive a tool-response message from role \"user\" containing a JSON object.\n",
      "The tool-response object always includes fields \"tool_name\" and \"status\".\n",
      "\n",
      "If the tool call succeeded, the tool response object will have the structure:\n",
      "{\"tool_name\": \"<tool_name>\", \"status\": \"ok\", \"result\": { ... }}\n",
      "Different tools have different structures of the returned data inside the dictionary \"result\".\n",
      "If you know what to do next, go ahead (e.g., if the user already told you what to do with the results, or if you have a plan and you want to use information from the results to do another tool call).\n",
      "If you're not sure what to do next or how to present the results to the user, you can respond to the user with \"o.k. what now?\" to get further instructions.\n",
      "\n",
      "If the tool call fails, the tool response object will have the structure:\n",
      "{\"tool_name\": \"<tool_name>\", \"status\": \"error\", \"error_message\": \" ... \"}\n",
      "If the error message is clear enough, you can try to fix the problem yourself (e.g., call the same tool with corrected arguments, or call another tool).\n",
      "Otherwise, you can surface the error message back to the user (with \"respond_to_user\") sto get further instructions.\n",
      "\n",
      "Available tools:\n",
      "\n",
      "search_phrase(phrase: str, n_max_results: int = 10) -> dict\n",
      "    Search the bible for all the verses that contain a specific phrase.\n",
      "    Currently supporting Hebrew text only (searching in WLCC version - Westminster Leningrad Codex (Consonants)).\n",
      "\n",
      "    Args:\n",
      "    - phrase (str): the word or phrase to search for. Currently only supporting a phrase in Hebrew without Nikkud (consonants only).\n",
      "    - n_max_results (int): the maximum number of results to return. Default: 10\n",
      "\n",
      "    Returns:\n",
      "    - dictionary with a field \"results\" of the verses that have the phrase - a list items, each is a dictionary with fields:\n",
      "        - book_name (str): the name of the biblical book\n",
      "        - chapter_num (int): the chapter number inside the book\n",
      "        - verse_num (int): the verse number inside the chapter\n",
      "        - text (str): the text of the found verse (this text should include the searched phrase as a substring)\n",
      "    \n",
      "\n",
      "----\n",
      "\n",
      "lookup_verse(version: str, book: str, chapter_num: int, verse_num: int) -> dict\n",
      "    Get the text of a specific verse from the bible.\n",
      "\n",
      "    Args:\n",
      "    - version (str): the code name of a specific bible version or translation\n",
      "    - book (str): the name of the book from the bible\n",
      "    - chapter_num (int): chapter number inside the book\n",
      "    - verse_num (int): verse number inside the chapter\n",
      "\n",
      "    Returns:\n",
      "    - dictionary with fields version, book, chapter_num, and verse_num coppied from the input arguments, and an additional field:\n",
      "        - text (str): the text of the requested verse\n",
      "    \n",
      "\n",
      "search for 'ואחיו' and show me just text examples for 9 results<end_of_turn>\n",
      "<start_of_turn>model\n",
      "{\"tool\": \"search_phrase\", \"arguments\": {\"phrase\": \"ואחיו\"}}<end_of_turn>\n",
      "<start_of_turn>user\n",
      "{\"tool_name\": \"search_phrase\", \"status\": \"ok\", \"result\": {\"results\": [{\"book_name\": \"Genesis\", \"chapter_num\": 44, \"verse_num\": 14, \"text\": \"ויבא יהודה ואחיו ביתה יוסף והוא עודנו שם ויפלו לפניו ארצה׃\"}, {\"book_name\": \"Genesis\", \"chapter_num\": 44, \"verse_num\": 20, \"text\": \"ונאמר אל־אדני יש־לנו אב זקן וילד זקנים קטן ואחיו מת ויותר הוא לבדו לאמו ואביו אהבו׃\"}, {\"book_name\": \"Genesis\", \"chapter_num\": 50, \"verse_num\": 8, \"text\": \"וכל בית יוסף ואחיו ובית אביו רק טפם וצאנם ובקרם עזבו בארץ גשן׃\"}, {\"book_name\": \"Genesis\", \"chapter_num\": 50, \"verse_num\": 14, \"text\": \"וישב יוסף מצרימה הוא ואחיו וכל־העלים אתו לקבר את־אביו אחרי קברו את־אביו׃\"}, {\"book_name\": \"Deuteronomy\", \"chapter_num\": 25, \"verse_num\": 11, \"text\": \"כי־ינצו אנשים יחדו איש ואחיו וקרבה אשת האחד להציל את־אישה מיד מכהו ושלחה ידה והחזיקה במבשיו׃\"}, {\"book_name\": \"Judges\", \"chapter_num\": 9, \"verse_num\": 26, \"text\": \"ויבא געל בן־עבד ואחיו ויעברו בשכם ויבטחו־בו בעלי שכם׃\"}, {\"book_name\": \"Judges\", \"chapter_num\": 9, \"verse_num\": 31, \"text\": \"וישלח מלאכים אל־אבימלך בתרמה לאמר הנה געל בן־עבד ואחיו באים שכמה והנם צרים את־העיר עליך׃\"}, {\"book_name\": \"2 Samuel\", \"chapter_num\": 6, \"verse_num\": 3, \"text\": \"וירכבו את־ארון האלהים אל־עגלה חדשה וישאהו מבית אבינדב אשר בגבעה ועזא ואחיו בני אבינדב נהגים את־העגלה חדשה׃\"}, {\"book_name\": \"2 Samuel\", \"chapter_num\": 6, \"verse_num\": 4, \"text\": \"וישאהו מבית אבינדב אשר בגבעה עם ארון האלהים ואחיו הלך לפני הארון׃\"}, {\"book_name\": \"1 Chronicles\", \"chapter_num\": 2, \"verse_num\": 16, \"text\": \"[ואחיתיהם כ] (ואחיותיהם ק) צרויה ואביגיל ובני צרויה אבשי ויואב ועשה־אל שלשה׃\"}, {\"book_name\": \"1 Chronicles\", \"chapter_num\": 5, \"verse_num\": 7, \"text\": \"ואחיו למשפחתיו בהתיחש לתלדותם הראש יעיאל וזכריהו׃\"}, {\"book_name\": \"1 Chronicles\", \"chapter_num\": 6, \"verse_num\": 24, \"text\": \"ואחיו אסף העמד על־ימינו אסף בן־ברכיהו בן־שמעא׃\"}, {\"book_name\": \"1 Chronicles\", \"chapter_num\": 8, \"verse_num\": 14, \"text\": \"ואחיו ששק וירמות׃\"}, {\"book_name\": \"1 Chronicles\", \"chapter_num\": 8, \"verse_num\": 31, \"text\": \"וגדור ואחיו וזכר׃\"}, {\"book_name\": \"1 Chronicles\", \"chapter_num\": 9, \"verse_num\": 19, \"text\": \"ושלום בן־קורא בן־אביסף בן־קרח  ואחיו לבית־אביו הקרחים על מלאכת העבודה שמרי הספים לאהל ואבתיהם על־מחנה יהוה שמרי המבוא׃\"}, {\"book_name\": \"1 Chronicles\", \"chapter_num\": 9, \"verse_num\": 37, \"text\": \"וגדור ואחיו וזכריה ומקלות׃\"}, {\"book_name\": \"1 Chronicles\", \"chapter_num\": 13, \"verse_num\": 7, \"text\": \"וירכיבו את־ארון האלהים על־עגלה חדשה מבית אבינדב ועזא ואחיו נהגים בעגלה׃\"}, {\"book_name\": \"1 Chronicles\", \"chapter_num\": 15, \"verse_num\": 5, \"text\": \"לבני קהת אוריאל השר ואחיו מאה ועשרים׃    ס\"}, {\"book_name\": \"1 Chronicles\", \"chapter_num\": 15, \"verse_num\": 6, \"text\": \"לבני מררי עשיה השר ואחיו מאתים ועשרים׃    ס\"}, {\"book_name\": \"1 Chronicles\", \"chapter_num\": 15, \"verse_num\": 7, \"text\": \"לבני גרשום יואל השר ואחיו מאה ושלשים׃    ס\"}]}}<end_of_turn>\n",
      "<start_of_turn>model\n",
      "{\"tool\": \"respond_to_user\", \"arguments\": {\"text\": \"ויבא יהודה ואחיו ביתה יוסף והוא עודנו שם ויפלו לפניו ארצה׃\\nונאמר אל־אדני יש־לנו אב זקן וילד זקנים קטן ואחיו מת ויותר הוא לבדו לאמו ואביו אהבו׃\\nוכל בית יוסף ואחיו ובית אביו רק טפם וצאנם ובקרם עזבו בארץ גשן׃\\nוישב יוסף מצרימה הוא ואחיו וכל־העלים אתו לקבר את־אביו אחרי קברו את־אביו׃\\nכי־ינצו אנשים יחדו איש ואחיו וקרבה אשת האחד להציל את־אישה מיד מכהו ושלחה ידה והחזיקה במבשיו׃\\nויבא געל בן־עבד ואחיו ויעברו בשכם ויבטחו־בו בעלי שכם׃\\nוישלח מלאכים אל־אבימלך בתרמה לאמר הנה געל בן־עבד ואחיו באים שכמה והנם צרים את־העיר עליך׃\\nוירכבו את־ארון האלהים אל־עגלה חדשה וישאהו מבית אבינדב אשר בגבעה ועזא ואחיו בני אבינדב נהגים את־העגלה חדשה׃\\nוישאהו מבית אבינדב אשר בגבעה עם ארון האלהים ואחיו הלך לפני הארון׃\"}}<end_of_turn>\n",
      "<start_of_turn>user\n",
      "Hello<end_of_turn>\n",
      "<start_of_turn>model\n",
      "{\"tool\": \"respond_to_user\", \"arguments\": {\"text\": \"Hi. How can I help you?\"}}<end_of_turn>\n",
      "\n",
      "========================================\n",
      "\n",
      "{\"tool\": \"search_phrase\", \"arguments\": {\"phrase\": \"ואחיו\"}}\n",
      "{\"tool\": \"respond_to_user\", \"arguments\": {\"text\": \"ויבא יהודה ואחיו ביתה יוסף והוא עודנו שם ויפלו לפניו ארצה׃\\nונאמר אל־אדני יש־לנו אב זקן וילד זקנים קטן ואחיו מת ויותר הוא לבדו לאמו ואביו אהבו׃\\nוכל בית יוסף ואחיו ובית אביו רק טפם וצאנם ובקרם עזבו בארץ גשן׃\\nוישב יוסף מצרימה הוא ואחיו וכל־העלים אתו לקבר את־אביו אחרי קברו את־אביו׃\\nכי־ינצו אנשים יחדו איש ואחיו וקרבה אשת האחד להציל את־אישה מיד מכהו ושלחה ידה והחזיקה במבשיו׃\\nויבא געל בן־עבד ואחיו ויעברו בשכם ויבטחו־בו בעלי שכם׃\\nוישלח מלאכים אל־אבימלך בתרמה לאמר הנה געל בן־עבד ואחיו באים שכמה והנם צרים את־העיר עליך׃\\nוירכבו את־ארון האלהים אל־עגלה חדשה וישאהו מבית אבינדב אשר בגבעה ועזא ואחיו בני אבינדב נהגים את־העגלה חדשה׃\\nוישאהו מבית אבינדב אשר בגבעה עם ארון האלהים ואחיו הלך לפני הארון׃\"}}\n",
      "{\"tool\": \"respond_to_user\", \"arguments\": {\"text\": \"Hi. How can I help you?\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Debugging code:\n",
    "import re\n",
    "\n",
    "def reverse_tokenized_data(tok_example):\n",
    "    labels = [107 if id<0 else id for id in tok_example['labels']]\n",
    "    text = tokenizer.decode(labels)\n",
    "    text = re.sub(\"\\n+\",\"\\n\", text)\n",
    "\n",
    "    whole_text = tokenizer.decode(tok_example['input_ids'])\n",
    "    return text, whole_text\n",
    "(label_text, whole_text) = reverse_tokenized_data(train_tok[81])\n",
    "print(whole_text)\n",
    "print(\"=\"*40)\n",
    "print(label_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed58c6",
   "metadata": {},
   "source": [
    "## Train with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7bf09b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yonatan\\Documents\\coding_projects\\bible-text-analysis\\models\\gemma3-2tools1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "print(output_dir)\n",
    "os.listdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "077aa012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma3ForCausalLM(\n",
       "      (model): Gemma3TextModel(\n",
       "        (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-25): 26 x Gemma3DecoderLayer(\n",
       "            (self_attn): Gemma3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1152, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1152, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1152, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1152, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=1024, out_features=1152, bias=False)\n",
       "              (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Gemma3MLP(\n",
       "              (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "              (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "              (down_proj): Linear(in_features=6912, out_features=1152, bias=False)\n",
       "              (act_fn): GELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (rotary_emb): Gemma3RotaryEmbedding()\n",
       "        (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3d5d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='354' max='426' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [354/426 21:05:32 < 4:18:51, 0.00 it/s, Epoch 1.66/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.397100</td>\n",
       "      <td>0.246724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.194700</td>\n",
       "      <td>0.159505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.140600</td>\n",
       "      <td>0.129712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.125200</td>\n",
       "      <td>0.107252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.096700</td>\n",
       "      <td>0.093277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.095700</td>\n",
       "      <td>0.084157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.080100</td>\n",
       "      <td>0.078716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 502 thrown while requesting HEAD https://huggingface.co/google/gemma-3-1b-it/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "HTTP Error 502 thrown while requesting HEAD https://huggingface.co/google/gemma-3-1b-it/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2649f288-a161-4b75-9d6b-927197893428)')' thrown while requesting HEAD https://huggingface.co/google/gemma-3-1b-it/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "HTTP Error 502 thrown while requesting HEAD https://huggingface.co/google/gemma-3-1b-it/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "HTTP Error 502 thrown while requesting HEAD https://huggingface.co/google/gemma-3-1b-it/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 1b7ba479-99a3-4def-83f8-5877e80ef18c)')' thrown while requesting HEAD https://huggingface.co/google/gemma-3-1b-it/resolve/main/config.json\n",
      "c:\\Users\\Yonatan\\Documents\\coding_projects\\bible-text-analysis\\.venv\\Lib\\site-packages\\peft\\utils\\other.py:1394: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 1b7ba479-99a3-4def-83f8-5877e80ef18c)') - silently ignoring the lookup for the file config.json in google/gemma-3-1b-it.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Yonatan\\Documents\\coding_projects\\bible-text-analysis\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:295: UserWarning: Could not find a config file in google/gemma-3-1b-it - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "#    gradient_accumulation_steps=1,### Debugging\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=50,\n",
    "#    logging_steps=1, ## Debugging\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "#    eval_steps=1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_only_model=True,\n",
    "    save_safetensors=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "use_train_set = train_tok\n",
    "use_eval_set = test_tok\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=use_train_set,\n",
    "    eval_dataset=use_eval_set,\n",
    "    data_collator=collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f7fd6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': 'You are a research assistant that always responds using a JSON object with fields \"tool\" and \"arguments\".\\n\\nTo respond normally to the user, use:\\n{\"tool\": \"respond_to_user\", \"arguments\":{\"text\": \"<text to show the user>\"}}\\n\\nTo call a tool, use:\\n{\"tool\": \"<tool_name>\", \"arguments\":{ ... }}\\n\\nAfter you call a tool, you will receive a message with role \"user\" containing a JSON object.\\nThe tool message always includes \"tool_name\" and \"status\".\\n\\nIf \"status\" is \"ok\":\\n- The message will include a \"result\" object.\\n- Read \"result.text\".\\n- Respond using \"respond_to_user\" and copy \"result.text\" exactly as-is.\\n\\nIf \"status\" is \"error\":\\n- The message will include an \"error_message\".\\n- If the error message is clear enough (e.g., if the user spelled a book name wrong and it is clear which book the user intended), you can call the tool again with the corrected arguments.\\n- Otherwise, respond using \"respond_to_user\" and copy \"error_message\" exactly as-is (to let the user tell you what to do next).\\n\\nRules:\\n- Never modify, translate, summarize, or explain text returned by the tool.\\n- Never add commentary or extra text.\\n- Never guess missing arguments.\\n- Never correct user mistakes on the first attempt of a tool call.\\n- Never retry a failed tool call with exactly the same arguments.\\n\\nAvailable tool:\\n\\nTo get the text of a specific biblical verse:\\n{\"tool\": \"lookup_verse\", \"arguments\":{\"version\":\"<version>\", book:\"<book>\", chapter_num:<integer>, verse_num:<integer>}}\\nWhen this tool call succeeds, contains the verse. Copy it exactly as-is and return to the user with \"respond_to_user\".\\n\\n'}\n",
      "{'role': 'user', 'content': \"Give me the verse located at isaiah 30:26 ('he.maworah').\"}\n",
      "{'role': 'assistant', 'content': '{\"tool\": \"lookup_verse\", \"arguments\": {\"version\": \"he.maworah\", \"book\": \"isaiah\", \"chapter_num\": 30, \"verse_num\": 26}}'}\n",
      "{'role': 'user', 'content': '{\"tool_name\": \"lookup_verse\", \"status\": \"error\", \"error_message\": \"We don\\'t support text-version named \\'he.maworah\\'. Here are the supported version:\\\\n  Version: \\'he.text_only\\'. Description: Hebrew language, only letters.\\\\n  Version: \\'he.masorah\\'. Description: Hebrew language, traditional script version (full markings).\\\\n  Version: \\'en.new.jps1917\\'. Description: English language, JPS.\\\\n  Version: \\'en.koren\\'. Description: English language, Koren.\"}'}\n",
      "----------------------------------------\n",
      "{\"tool\": \"respond_to_user\", \"arguments\": {\"text\": \"We don't support text-version named 'he.maworah'. Here are the supported version:\\n  Version: 'he.text_only'. Description: Hebrew language, only letters.\\n  Version: 'he.masorah'. Description: Hebrew language, traditional script version (full markings).\\n  Version: 'en.new.jps1917'. Description: English language\n"
     ]
    }
   ],
   "source": [
    "input_messages = test_ds[11]['messages'][:4]\n",
    "for msg in input_messages:\n",
    "    print(msg)\n",
    "print(\"-\"*40)\n",
    "resp = do_generate_response(tokenizer, model, input_messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ead25fb",
   "metadata": {},
   "source": [
    "## Merge, save, and register with ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa059f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lora_checkpoint(base_model_name, training_dir, checkpoint):\n",
    "    adapter_dir = f\"{training_dir}/checkpoint-{checkpoint}\"\n",
    "    merged_dir = f\"{training_dir}/merged-checkpoint-{checkpoint}\"\n",
    "\n",
    "    # Keep in mind you need to get a fresh base every time before attaching to the adaptation (otherwise you may mutate the same model with different adaptations)\n",
    "    base_tmp = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "    adapted = PeftModel.from_pretrained(base_tmp, adapter_dir)\n",
    "    merged = adapted.merge_and_unload()\n",
    "    merged.save_pretrained(merged_dir)\n",
    "    tokenizer.save_pretrained(merged_dir)\n",
    "    return merged_dir\n",
    "\n",
    "def write_gemma_modelfile(merged_dir, ollama_base_model_name):\n",
    "    \"\"\"\n",
    "    Create a correct Modelfile for a fine‑tuned Gemma model by copying the\n",
    "    base model's Modelfile and replacing only the FROM line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    merged_dir : str or Path\n",
    "        Path to the folder containing the merged model files.\n",
    "    ollama_base_model_name : str\n",
    "        Name of the base model as registered in Ollama (e.g., \"gemma3:1b\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        Path to the newly written Modelfile.\n",
    "    \"\"\"\n",
    "\n",
    "    merged_dir = Path(merged_dir)\n",
    "    modelfile_path = merged_dir / \"Modelfile\"\n",
    "\n",
    "    # 1. Get the base model's Modelfile text\n",
    "    result = subprocess.run(\n",
    "        [\"ollama\", \"show\", \"--modelfile\", ollama_base_model_name],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=True\n",
    "    )\n",
    "    base_modelfile = result.stdout\n",
    "\n",
    "    # 2. Replace the FROM line with a local directory reference\n",
    "    new_from = f\"FROM {merged_dir.as_posix()}\"\n",
    "    modelfile_lines = base_modelfile.splitlines()\n",
    "\n",
    "    for i, line in enumerate(modelfile_lines):\n",
    "        if line.startswith(\"FROM \"):\n",
    "            modelfile_lines[i] = new_from\n",
    "            break\n",
    "\n",
    "    updated_modelfile = \"\\n\".join(modelfile_lines)\n",
    "\n",
    "    # 3. Write the updated Modelfile into merged_dir\n",
    "    with open(modelfile_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(updated_modelfile)\n",
    "\n",
    "    return modelfile_path\n",
    "\n",
    "def build_ollama_model_cli(model_name: str, merged_dir: str):\n",
    "    merged_dir = Path(merged_dir)\n",
    "    modelfile = merged_dir / \"Modelfile\"\n",
    "\n",
    "    subprocess.run(\n",
    "        [\"ollama\", \"create\", model_name, \"-f\", str(modelfile)],\n",
    "        check=True\n",
    "    )\n",
    "\n",
    "def merge_save_ollama(base_model_name, training_dir, checkpoint, ollama_base_model_name, model_nickname):\n",
    "    merged_dir = merge_lora_checkpoint(base_model_name, training_dir, checkpoint)\n",
    "    print(f\"Merged model folder: {merged_dir}\")\n",
    "    print(os.listdir(merged_dir))\n",
    "\n",
    "    modelfile_path = write_gemma_modelfile(merged_dir, ollama_base_model_name)\n",
    "    print(f\"==> Wrote Modelfile: {modelfile_path}\")\n",
    "\n",
    "    new_model_name = f\"{ollama_base_model_name}-{model_nickname}-{checkpoint}\"\n",
    "    build_ollama_model_cli(new_model_name, merged_dir)\n",
    "    print(f\"Registered {new_model_name} in ollama. Now ollama has:\")\n",
    "    print(ollama.list())\n",
    "    return (merged_dir, new_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a0387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model folder: c:\\Users\\Yonatan\\Documents\\coding_projects\\bible-text-analysis\\models\\gemma3-lora-9/merged-checkpoint-350\n",
      "['added_tokens.json', 'chat_template.jinja', 'config.json', 'generation_config.json', 'model.safetensors', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json']\n",
      "==> Wrote Modelfile: c:\\Users\\Yonatan\\Documents\\coding_projects\\bible-text-analysis\\models\\gemma3-lora-9\\merged-checkpoint-350\\Modelfile\n",
      "Registered gemma3:1b-ft9-350 in ollama. Now ollama has:\n",
      "models=[Model(model='gemma3:1b-ft9-350', modified_at=datetime.datetime(2026, 1, 28, 9, 16, 5, 457480, tzinfo=TzInfo(-18000)), digest='8d25ff0f607b75234689b0a8a1d8104ab6895a444845c699068b889785fdf89c', size=2015833891, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')), Model(model='gemma3:1b-ft3-50', modified_at=datetime.datetime(2026, 1, 23, 13, 54, 9, 433956, tzinfo=TzInfo(-18000)), digest='9daf2271960acc2503ed4f895bbefdf85ea4b0d77e18348fedf6be476d8bfbab', size=2015833891, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')), Model(model='gemma3:1b-ft3-200', modified_at=datetime.datetime(2026, 1, 23, 13, 48, 22, 338494, tzinfo=TzInfo(-18000)), digest='19a72e403cba4e546672312fe3add372f266bbcfd205e4d9f76b30d9e9a97ca8', size=2015833891, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')), Model(model='gemma3:1b-ft3', modified_at=datetime.datetime(2026, 1, 23, 13, 39, 57, 531804, tzinfo=TzInfo(-18000)), digest='f29938c31eef2f0966c7148ba9630307fa0381f21d5fbcfacbb43ba872d3840e', size=2015833891, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')), Model(model='gemma3:1b-ft2', modified_at=datetime.datetime(2026, 1, 23, 11, 51, 6, 306399, tzinfo=TzInfo(-18000)), digest='252321fba99d14a9cba449fa8f045c92e97bdec3fadcbfa9ab446766caca55a8', size=2015833891, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')), Model(model='gemma3:1b-ft1', modified_at=datetime.datetime(2026, 1, 16, 10, 38, 25, 375681, tzinfo=TzInfo(-18000)), digest='f246014b90b8cb36ca293a97c7b5c5ce2e9cfa4790050d6a659c02401dca8c9c', size=2015833891, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')), Model(model='google/gemma-3-1b-it-ft1:latest', modified_at=datetime.datetime(2026, 1, 16, 10, 8, 13, 449228, tzinfo=TzInfo(-18000)), digest='b615ad8f3a73023d47771e856d21f4069d053d3e745b000a5752f5b858b0586d', size=2015825284, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')), Model(model='gemma3:4b', modified_at=datetime.datetime(2026, 1, 14, 9, 23, 35, 705701, tzinfo=TzInfo(-18000)), digest='a2af6cc3eb7fa8be8504abaf9b04e88f17a119ec3f04a3addf55f92841195f5a', size=3338801804, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='4.3B', quantization_level='Q4_K_M')), Model(model='llama3.2:3b', modified_at=datetime.datetime(2026, 1, 7, 13, 40, 29, 314297, tzinfo=TzInfo(-18000)), digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', size=2019393189, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M')), Model(model='llama3.2:1b', modified_at=datetime.datetime(2026, 1, 7, 13, 39, 39, 839186, tzinfo=TzInfo(-18000)), digest='baf6a787fdffd633537aa2eb51cfd54cb93ff08e28040095462bb63daf552878', size=1321098329, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='1.2B', quantization_level='Q8_0')), Model(model='gemma3:1b', modified_at=datetime.datetime(2025, 12, 15, 16, 18, 47, 663104, tzinfo=TzInfo(-18000)), digest='8648f39daa8fbf5b18c7b4e6a8fb4990c692751d49917417b8842ca5758e7ffc', size=815319791, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='Q4_K_M')), Model(model='mistral:latest', modified_at=datetime.datetime(2025, 12, 9, 19, 37, 27, 232056, tzinfo=TzInfo(-18000)), digest='6577803aa9a036369e481d648a2baebb381ebc6e897f2bb9a766a2aa7bfbc1cf', size=4372824384, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='7.2B', quantization_level='Q4_K_M'))]\n"
     ]
    }
   ],
   "source": [
    "(merged_dir, new_model_name) = merge_save_ollama(model_name, output_dir, 350, ollama_base_model_name, model_nickname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34dde0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model folder: c:\\Users\\Yonatan\\Documents\\coding_projects\\bible-text-analysis\\models\\gemma3-lora-9/merged-checkpoint-650\n",
      "['added_tokens.json', 'chat_template.jinja', 'config.json', 'generation_config.json', 'model.safetensors', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json']\n",
      "==> Wrote Modelfile: c:\\Users\\Yonatan\\Documents\\coding_projects\\bible-text-analysis\\models\\gemma3-lora-9\\merged-checkpoint-650\\Modelfile\n",
      "Registered gemma3:1b-ft9-650 in ollama. Now ollama has:\n",
      "models=[Model(model='gemma3:1b-ft9-650', modified_at=datetime.datetime(2026, 1, 28, 9, 17, 20, 621636, tzinfo=TzInfo(-18000)), digest='c5a1be2c32546bb87caedb2bf083dc2cf574f767233460d3cb0dd6d4f6e9e866', size=2015833891, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')), Model(model='gemma3:1b-ft9-350', modified_at=datetime.datetime(2026, 1, 28, 9, 16, 5, 457480, tzinfo=TzInfo(-18000)), digest='8d25ff0f607b75234689b0a8a1d8104ab6895a444845c699068b889785fdf89c', size=2015833891, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')), Model(model='gemma3:1b-ft3-50', modified_at=datetime.datetime(2026, 1, 23, 13, 54, 9, 433956, tzinfo=TzInfo(-18000)), digest='9daf2271960acc2503ed4f895bbefdf85ea4b0d77e18348fedf6be476d8bfbab', size=2015833891, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')), Model(model='gemma3:1b-ft3-200', modified_at=datetime.datetime(2026, 1, 23, 13, 48, 22, 338494, tzinfo=TzInfo(-18000)), digest='19a72e403cba4e546672312fe3add372f266bbcfd205e4d9f76b30d9e9a97ca8', size=2015833891, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')), Model(model='gemma3:1b-ft3', modified_at=datetime.datetime(2026, 1, 23, 13, 39, 57, 531804, tzinfo=TzInfo(-18000)), digest='f29938c31eef2f0966c7148ba9630307fa0381f21d5fbcfacbb43ba872d3840e', size=2015833891, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')), Model(model='gemma3:1b-ft2', modified_at=datetime.datetime(2026, 1, 23, 11, 51, 6, 306399, tzinfo=TzInfo(-18000)), digest='252321fba99d14a9cba449fa8f045c92e97bdec3fadcbfa9ab446766caca55a8', size=2015833891, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')), Model(model='gemma3:1b-ft1', modified_at=datetime.datetime(2026, 1, 16, 10, 38, 25, 375681, tzinfo=TzInfo(-18000)), digest='f246014b90b8cb36ca293a97c7b5c5ce2e9cfa4790050d6a659c02401dca8c9c', size=2015833891, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')), Model(model='google/gemma-3-1b-it-ft1:latest', modified_at=datetime.datetime(2026, 1, 16, 10, 8, 13, 449228, tzinfo=TzInfo(-18000)), digest='b615ad8f3a73023d47771e856d21f4069d053d3e745b000a5752f5b858b0586d', size=2015825284, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')), Model(model='gemma3:4b', modified_at=datetime.datetime(2026, 1, 14, 9, 23, 35, 705701, tzinfo=TzInfo(-18000)), digest='a2af6cc3eb7fa8be8504abaf9b04e88f17a119ec3f04a3addf55f92841195f5a', size=3338801804, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='4.3B', quantization_level='Q4_K_M')), Model(model='llama3.2:3b', modified_at=datetime.datetime(2026, 1, 7, 13, 40, 29, 314297, tzinfo=TzInfo(-18000)), digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', size=2019393189, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M')), Model(model='llama3.2:1b', modified_at=datetime.datetime(2026, 1, 7, 13, 39, 39, 839186, tzinfo=TzInfo(-18000)), digest='baf6a787fdffd633537aa2eb51cfd54cb93ff08e28040095462bb63daf552878', size=1321098329, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='1.2B', quantization_level='Q8_0')), Model(model='gemma3:1b', modified_at=datetime.datetime(2025, 12, 15, 16, 18, 47, 663104, tzinfo=TzInfo(-18000)), digest='8648f39daa8fbf5b18c7b4e6a8fb4990c692751d49917417b8842ca5758e7ffc', size=815319791, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='Q4_K_M')), Model(model='mistral:latest', modified_at=datetime.datetime(2025, 12, 9, 19, 37, 27, 232056, tzinfo=TzInfo(-18000)), digest='6577803aa9a036369e481d648a2baebb381ebc6e897f2bb9a766a2aa7bfbc1cf', size=4372824384, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='7.2B', quantization_level='Q4_K_M'))]\n"
     ]
    }
   ],
   "source": [
    "(merged_dir, new_model_name) = merge_save_ollama(model_name, output_dir, 650, ollama_base_model_name, model_nickname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7033c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa9d5637",
   "metadata": {},
   "source": [
    "## Test the merged model using huggingface (not ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da3e11cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_hf_model = AutoModelForCausalLM.from_pretrained(merged_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be316771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': 'You are a research assistant that always responds using a JSON object with fields \"tool\" and \"arguments\".\\n\\nTo respond normally to the user, use:\\n{\"tool\": \"respond_to_user\", \"arguments\":{\"text\": \"<text to show the user>\"}}\\n\\nTo call a tool, use:\\n{\"tool\": \"<tool_name>\", \"arguments\":{ ... }}\\n\\nAfter you call a tool, you will receive a message with role \"user\" containing a JSON object.\\nThe tool message always includes \"tool_name\" and \"status\".\\n\\nIf \"status\" is \"ok\":\\n- The message will include a \"result\" object.\\n- Read \"result.text\".\\n- Respond using \"respond_to_user\" and copy \"result.text\" exactly as-is.\\n\\nIf \"status\" is \"error\":\\n- The message will include an \"error_message\".\\n- If the error message is clear enough (e.g., if the user spelled a book name wrong and it is clear which book the user intended), you can call the tool again with the corrected arguments.\\n- Otherwise, respond using \"respond_to_user\" and copy \"error_message\" exactly as-is (to let the user tell you what to do next).\\n\\nRules:\\n- Never modify, translate, summarize, or explain text returned by the tool.\\n- Never add commentary or extra text.\\n- Never guess missing arguments.\\n- Never correct user mistakes on the first attempt of a tool call.\\n- Never retry a failed tool call with exactly the same arguments.\\n\\nAvailable tool:\\n\\nTo get the text of a specific biblical verse:\\n{\"tool\": \"lookup_verse\", \"arguments\":{\"version\":\"<version>\", book:\"<book>\", chapter_num:<integer>, verse_num:<integer>}}\\nWhen this tool call succeeds, contains the verse. Copy it exactly as-is and return to the user with \"respond_to_user\".\\n\\n'}\n",
      "{'role': 'user', 'content': \"Give me the verse located at isaiah 44:14 ('he.masorah').\"}\n",
      "{'role': 'assistant', 'content': '{\"tool\": \"lookup_verse\", \"arguments\": {\"version\": \"he.masorah\", \"book\": \"isaiah\", \"chapter_num\": 44, \"verse_num\": 14}}'}\n",
      "{'role': 'user', 'content': '{\"tool_name\": \"lookup_verse\", \"status\": \"ok\", \"result\": {\"version\": \"he.masorah\", \"book\": \"isaiah\", \"chapter_num\": 44, \"verse_num\": 14, \"text\": \"לִכְרׇת־ל֣וֹ אֲרָזִ֔ים וַיִּקַּ֤ח תִּרְזָה֙ וְאַלּ֔וֹן וַיְאַמֶּץ־ל֖וֹ בַּעֲצֵי־יָ֑עַר נָטַ֥ע אֹ֖רֶןוְגֶ֥שֶׁם יְגַדֵּֽל׃\"}}'}\n",
      "----------------------------------------\n",
      "{\"tool\": \"respond_to_user\", \"arguments\": {\"text\": \"לִכְרׇת־ל֣וֹ אֲרָזִ֔ים וַיִּקַּ֤ח תִּרְזָה֙ וְאַלּ֔וֹן וַיְאַמֶּץ־ל֖וֹ בַּעֲצֵי־יָ֑עַר נָטַ֥\n"
     ]
    }
   ],
   "source": [
    "input_messages = test_ds[0]['messages'][:4]\n",
    "for msg in input_messages:\n",
    "    print(msg)\n",
    "print(\"-\"*40)\n",
    "resp = do_generate_response(tokenizer, loaded_hf_model, input_messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "033c1ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(model='gemma3:1b-ft9-650', modified_at=datetime.datetime(2026, 1, 28, 9, 17, 20, 621636, tzinfo=TzInfo(-18000)), digest='c5a1be2c32546bb87caedb2bf083dc2cf574f767233460d3cb0dd6d4f6e9e866', size=2015833891, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')),\n",
       " Model(model='gemma3:1b-ft9-350', modified_at=datetime.datetime(2026, 1, 28, 9, 16, 5, 457480, tzinfo=TzInfo(-18000)), digest='8d25ff0f607b75234689b0a8a1d8104ab6895a444845c699068b889785fdf89c', size=2015833891, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')),\n",
       " Model(model='gemma3:1b-ft3-50', modified_at=datetime.datetime(2026, 1, 23, 13, 54, 9, 433956, tzinfo=TzInfo(-18000)), digest='9daf2271960acc2503ed4f895bbefdf85ea4b0d77e18348fedf6be476d8bfbab', size=2015833891, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')),\n",
       " Model(model='gemma3:1b-ft3-200', modified_at=datetime.datetime(2026, 1, 23, 13, 48, 22, 338494, tzinfo=TzInfo(-18000)), digest='19a72e403cba4e546672312fe3add372f266bbcfd205e4d9f76b30d9e9a97ca8', size=2015833891, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')),\n",
       " Model(model='gemma3:1b-ft3', modified_at=datetime.datetime(2026, 1, 23, 13, 39, 57, 531804, tzinfo=TzInfo(-18000)), digest='f29938c31eef2f0966c7148ba9630307fa0381f21d5fbcfacbb43ba872d3840e', size=2015833891, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')),\n",
       " Model(model='gemma3:1b-ft2', modified_at=datetime.datetime(2026, 1, 23, 11, 51, 6, 306399, tzinfo=TzInfo(-18000)), digest='252321fba99d14a9cba449fa8f045c92e97bdec3fadcbfa9ab446766caca55a8', size=2015833891, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')),\n",
       " Model(model='gemma3:1b-ft1', modified_at=datetime.datetime(2026, 1, 16, 10, 38, 25, 375681, tzinfo=TzInfo(-18000)), digest='f246014b90b8cb36ca293a97c7b5c5ce2e9cfa4790050d6a659c02401dca8c9c', size=2015833891, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')),\n",
       " Model(model='google/gemma-3-1b-it-ft1:latest', modified_at=datetime.datetime(2026, 1, 16, 10, 8, 13, 449228, tzinfo=TzInfo(-18000)), digest='b615ad8f3a73023d47771e856d21f4069d053d3e745b000a5752f5b858b0586d', size=2015825284, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='F16')),\n",
       " Model(model='gemma3:4b', modified_at=datetime.datetime(2026, 1, 14, 9, 23, 35, 705701, tzinfo=TzInfo(-18000)), digest='a2af6cc3eb7fa8be8504abaf9b04e88f17a119ec3f04a3addf55f92841195f5a', size=3338801804, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='4.3B', quantization_level='Q4_K_M')),\n",
       " Model(model='llama3.2:3b', modified_at=datetime.datetime(2026, 1, 7, 13, 40, 29, 314297, tzinfo=TzInfo(-18000)), digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', size=2019393189, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M')),\n",
       " Model(model='llama3.2:1b', modified_at=datetime.datetime(2026, 1, 7, 13, 39, 39, 839186, tzinfo=TzInfo(-18000)), digest='baf6a787fdffd633537aa2eb51cfd54cb93ff08e28040095462bb63daf552878', size=1321098329, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='1.2B', quantization_level='Q8_0')),\n",
       " Model(model='gemma3:1b', modified_at=datetime.datetime(2025, 12, 15, 16, 18, 47, 663104, tzinfo=TzInfo(-18000)), digest='8648f39daa8fbf5b18c7b4e6a8fb4990c692751d49917417b8842ca5758e7ffc', size=815319791, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='Q4_K_M')),\n",
       " Model(model='mistral:latest', modified_at=datetime.datetime(2025, 12, 9, 19, 37, 27, 232056, tzinfo=TzInfo(-18000)), digest='6577803aa9a036369e481d648a2baebb381ebc6e897f2bb9a766a2aa7bfbc1cf', size=4372824384, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='7.2B', quantization_level='Q4_K_M'))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list().models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc70fcb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
